{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011fa5b7-0e9c-4985-86e3-125bbf281bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2023-present Ryan Balshaw under the MIT License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31320dd-5aa3-45df-95af-9b80907f1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define imports\n",
    "import scipy as spy\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "import spectrally_regularised_lvms as srLVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48718b04-ace5-46ea-95f5-c83cc481b306",
   "metadata": {},
   "source": [
    "# Implement a simple FFT function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a6302-0576-4a63-8619-8dd07b97bb38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple FFT function\n",
    "def fft_vis(x, Fs):\n",
    "    n = len(x)\n",
    "\n",
    "    # Cut in half at Nyquist frequency\n",
    "    fft_freq = np.fft.fftfreq(n, 1/Fs)[:n//2]\n",
    "    fft_val = 2/n * np.abs(np.fft.fft(x))[:n//2]\n",
    "    \n",
    "    return fft_freq, fft_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee4989-6c7c-44e1-966b-d69abee815b0",
   "metadata": {},
   "source": [
    "# Define the properties of the two parameter vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246361d4-3995-4821-bcb3-188e2c50a6a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ddim = 40 # Dimensionality of the problem\n",
    "Fs = 1.0 # The sampling frequency\n",
    "alpha_reg = 1.0 # Set to one for this example\n",
    "\n",
    "w_i = np.random.randn(Ddim, 1)  # Random vector\n",
    "w_i /= np.linalg.norm(w_i) # Normalised (not strictly necessary here)\n",
    "\n",
    "w_j = np.zeros((Ddim, 1)) # Broadband vector (excites all frequencies equally)\n",
    "w_j[0, 0] = 1 \n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 6))\n",
    "ax[0].plot(*fft_vis(w_i[:, 0], Fs), color = \"b\", label = r\"$\\mathbf{w}_i$\")\n",
    "ax[1].plot(*fft_vis(w_j[:, 0], Fs), color = \"r\", label = r\"$\\mathbf{w}_j$\")\n",
    "\n",
    "for axs in ax:\n",
    "    axs.grid()\n",
    "    axs.set_xlabel(\"Frequency\")\n",
    "    axs.set_ylabel(\"Magnitude\")\n",
    "    axs.grid(visible = True)\n",
    "    axs.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6668740d-70e7-4c90-8438-080e37a3ceda",
   "metadata": {},
   "source": [
    "# Paper derivation\n",
    "\n",
    "Please run all cells below this one sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14774cfe-8879-4d37-89ce-a97ab4ec71b9",
   "metadata": {},
   "source": [
    "## b vector (Represents squared spectral amplitude)\n",
    "$$ \\mathbf{b}(\\mathbf{w}) = \\frac{1}{D} \\left( \\mathbf{R} \\mathbf{w} \\odot \\mathbf{R} \\mathbf{w} + \\mathbf{I} \\mathbf{w} \\odot \\mathbf{I} \\mathbf{w}  \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58bab39-9e5c-40fe-a0b1-90e35818d40e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "D_matrix = spy.linalg.dft(Ddim, scale = None) # unscaled matrix\n",
    "\n",
    "def b_vector(D, w):\n",
    "    Ddim = w.shape[0]\n",
    "    \n",
    "    return 1/Ddim * np.abs(D @ w)**2 # More efficient implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7943a3-3861-458c-9dd6-cc289f3f07db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compute loss\n",
    "\n",
    "$$ \\mathcal{L}_{sr}(\\mathbf{w}_i) = \\alpha \\sum_{j=1}^{i-1} h(\\mathbf{w}_{i}, \\mathbf{w}_{j}),$$\n",
    "where \n",
    "$$h(\\mathbf{w}_{i}, \\mathbf{w}_{j}) = \\mathbf{b}(\\mathbf{w}_i)^T \\mathbf{b}(\\mathbf{w}_j).$$\n",
    "For this example, $i=2$ and $j = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc371ce-0563-4fe8-8393-e8cb4e60c7de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def h_loss(D, w_i, w_j):\n",
    "    \n",
    "    b_wi = b_vector(D, w_i)\n",
    "    b_wj = b_vector(D, w_j)\n",
    "    \n",
    "    return alpha_reg * (b_wi.T @ b_wj)[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1623df-49fd-4b7d-a013-5fbbe4520c51",
   "metadata": {},
   "source": [
    "## Compute the gradient vector\n",
    "\n",
    "$$ \\nabla_{\\mathbf{w}_i} \\mathcal{L}_{sr}(\\mathbf{w}_i) = \\frac{2 \\cdot \\alpha}{D} \\sum_{j=1}^{i - 1} \\left[ \\text{diag}(\\mathbf{R} \\mathbf{w}_i)\\mathbf{R} +  \\text{diag}(\\mathbf{I} \\mathbf{w}_i)\\mathbf{I} \\right]^T \\mathbf{b}(\\mathbf{w}_j) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80874d2-5fd7-4d7c-8b47-3b54d243b1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def h_grad(D, w_i, w_j):\n",
    "    \n",
    "    Ddim = D.shape[0]\n",
    "    \n",
    "    Re = np.real(D)\n",
    "    Im = np.imag(D)\n",
    "    \n",
    "    b_wj = b_vector(D, w_j) # b_wj carries a 1/D factor already.\n",
    "    \n",
    "    grad = ((2 * alpha_reg) / Ddim) * (np.diag((Re @ w_i)[:, 0]) @ Re + np.diag((Im @ w_i)[:, 0]) @ Im).T @  b_wj\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ff31f-2871-4bd3-9349-bd65ec0ef419",
   "metadata": {},
   "source": [
    "# Compute the Hessian matrix\n",
    "\n",
    "$$ \\mathbf{H}_{sr} =  \\frac{2\\cdot \\alpha}{D} \\sum_{j=1}^{i - 1} \\begin{bmatrix} \\mathbf{b}(\\mathbf{w}_j)^T \\left( \\text{diag}\\left(\\mathbf{r}_{1}\\right) \\mathbf{R} +  \\text{diag}\\left(\\mathbf{i}_{1}\\right) \\mathbf{I} \\right) \\\\ \\vdots \\\\ \\mathbf{b}(\\mathbf{w}_j)^T \\left( \\text{diag}\\left(\\mathbf{r}_{D}\\right) \\mathbf{R} + \\text{diag}\\left(\\mathbf{i}_{D}\\right) \\mathbf{I} \\right)\\end{bmatrix}.  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1de87-ef63-4cbe-89e7-a770f661612a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def h_hess(D, w_i, w_j):\n",
    "    \n",
    "    Ddim = D.shape[0]\n",
    "    \n",
    "    hess = np.zeros((Ddim, Ddim))\n",
    "    \n",
    "    Re = np.real(D)\n",
    "    Im = np.imag(D)\n",
    "    \n",
    "    b_wj = b_vector(D, w_j) # b_wj carries a 1/D factor already.\n",
    "    \n",
    "    for i in range(Ddim):\n",
    "        r_i = Re[:, [i]]\n",
    "        i_i = Im[:, [i]]\n",
    "        \n",
    "        hess_row = b_wj.T @ (np.diag(r_i[:, 0]) @ Re + np.diag(i_i[:, 0]) @ Im)\n",
    "        \n",
    "        hess[i, :] = hess_row\n",
    "        \n",
    "    return ((2 * alpha_reg) / Ddim) *  np.array(hess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7aa0a3-cddf-4dc2-b010-d33af5f4e653",
   "metadata": {},
   "source": [
    "# Sympy derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374a2f46-5c22-49ac-b8b4-9410a4f961e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string_w_sp = [f\"w_{i}\" for i in range(Ddim)] # Create a list w variable strings\n",
    "\n",
    "symbols = sp.symbols(\",\".join(string_w_sp)) # Create the SymPy symbols\n",
    "vals = [(s_i, w_i_val) for s_i, w_i_val in zip(symbols, w_i[:, 0])] # Create the substitution values\n",
    "\n",
    "w_i_sp = sp.Matrix(symbols) # Symbolic w_i vector\n",
    "w_j_sp = sp.Matrix(w_j[:, 0]) # w_j vector\n",
    "\n",
    "# Split into real and imaginary DFT matrix components for sympy version\n",
    "Re_sp = sp.Matrix(np.real(D_matrix)) \n",
    "Im_sp = sp.Matrix(np.imag(D_matrix))\n",
    "\n",
    "# Check sizes to ensure that everything is fine\n",
    "print(w_i_sp.shape)\n",
    "print(w_j_sp.shape)\n",
    "print(Re_sp.shape)\n",
    "print(Im_sp.shape)\n",
    "\n",
    "# Check that w_i_sp is a column vector of variables\n",
    "display(w_i_sp.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991473e-4c2c-4e3a-bcb5-7e3184ed287f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note, running this cell takes a while because of the .applyfunc(). \n",
    "# Decreasing Ddim in the `Define the properties..' cell reduces the solution time.\n",
    "\n",
    "def b_vector_sp(Re, Im, w): # Define the b_vector using sympy methods\n",
    "\n",
    "    Ddim = w.shape[0]\n",
    "    \n",
    "    Rw = Re @ w\n",
    "    Iw = Im @ w\n",
    "    \n",
    "    return 1/Ddim * (Rw.applyfunc(lambda x: x**2) + Iw.applyfunc(lambda x: x**2)) # easiest way to compute Hadamard product\n",
    "\n",
    "def loss_sp_func(Re, Im, w_i, w_j): # Define the loss\n",
    "    \n",
    "    Xw_i = b_vector_sp(Re, Im, w_i)\n",
    "    Xw_j = b_vector_sp(Re, Im, w_j)\n",
    "    \n",
    "    return Xw_i.T @ Xw_j\n",
    "\n",
    "loss_sp = loss_sp_func(Re_sp, Im_sp, w_i_sp, w_j_sp)[0, 0]\n",
    "grad_sp = sp.derive_by_array(loss_sp, symbols)\n",
    "hess_sp = sp.derive_by_array(grad_sp, symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e377a41b-7a66-4c2c-93bc-f45f1b57859f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visualise comparison\n",
    "\n",
    "In this comparison, we check the implementations of the b_vector for the implementation in this notebook and the implementation given in the *spectrally-regularised-LVMs* package against the SymPy implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735646d-916b-4631-a4b5-b59c89424eff",
   "metadata": {},
   "source": [
    "## Initialise the spectral objective from the Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef030db-1ba4-4e8c-a0d7-549b6b833f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialise spectral objective\n",
    "spectral_reg = srLVMs.SpectralObjective(Ddim, \n",
    "                                        save_hessian_flag = True, \n",
    "                                        inv_hessian_flag = False)\n",
    "\n",
    "# Get the Real and Imaginary components of the DFT matrix\n",
    "Re_code, Im_code = spectral_reg.decompose_DFT()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06296c9f-0f0b-4916-891c-c53cf30b5fe8",
   "metadata": {},
   "source": [
    "## Check $\\mathbf{b}(\\mathbf{w}_i)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad78f1d-ba46-490e-a10f-4214ee10dec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the b vector using the notebook implementation\n",
    "b_local = b_vector(D_matrix, w_i)[:, 0]\n",
    "\n",
    "# Get the b vector using the package implementation \n",
    "b_code = spectral_reg.Xw(Re_code, Im_code, w_i)[:, 0]\n",
    "\n",
    "# Get the b vector using the \n",
    "b_sympy = b_vector_sp(Re_sp, Im_sp, w_i_sp).subs(vals)\n",
    "b_sympy = np.array(b_sympy).astype(np.float64)[:, 0] # Convert to numpy\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize = (6, 10))\n",
    "ax[0].plot(b_local, color = \"b\", label = \"Notebook implementation\")\n",
    "ax[1].plot(b_code, color = \"r\", label = \"Package implementation\")\n",
    "ax[2].plot(b_sympy, color = \"m\", label = \"SymPy implementation\")\n",
    "\n",
    "for axs in ax:\n",
    "    axs.set_xlabel(r\"Vector indices ($i$)\")\n",
    "    axs.set_ylabel(r\"$b_i(\\mathbf{w}_i)$\")\n",
    "    axs.legend()\n",
    "    axs.grid(visible = True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Vector norm (notebook implementation): {np.linalg.norm(b_local - b_sympy)}\")\n",
    "print(f\"Vector norm (package implementation): {np.linalg.norm(b_code - b_sympy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a75e7c-42df-48de-893a-4103d1f25d3f",
   "metadata": {},
   "source": [
    "## Check the loss function values for the three implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1602a1d3-8a01-4276-8437-0b011596ad50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_local = h_loss(D_matrix, w_i, w_j)\n",
    "loss_code = spectral_reg.spectral_loss(w_i, w_j)\n",
    "loss_sp_vals = loss_sp.subs(vals)\n",
    "\n",
    "print(f\"Notebook implementation: {loss_local}\")\n",
    "print(f\"Package implementation: {loss_code}\")\n",
    "print(f\"SymPy implementation: {loss_sp_vals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5de679-5129-4c27-bb9d-48d164c0f0d8",
   "metadata": {},
   "source": [
    "## Check the gradient vectors for the three implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17f582-0196-426d-aadd-f77cb3d792f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grad_local = h_grad(D_matrix, w_i, w_j)[:, 0]\n",
    "grad_code = spectral_reg.spectral_derivative(w_i, w_j)\n",
    "grad_sp_vals = grad_sp.subs(vals)\n",
    "grad_sp_vals = np.array(grad_sp_vals).astype(np.float64)\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize = (6, 10))\n",
    "ax[0].plot(grad_local, color = \"b\", label = \"Notebook implementation\")\n",
    "ax[1].plot(grad_code, color = \"r\", label = \"Package implementation\")\n",
    "ax[2].plot(grad_sp_vals, color = \"m\", label = \"SymPy implementation\")\n",
    "\n",
    "for axs in ax:\n",
    "    axs.set_xlabel(r\"Gradient indices ($i$)\")\n",
    "    axs.set_ylabel(r\"$\\nabla_{\\mathbf{w}_i} \\mathcal{L}_{sr}$\")\n",
    "    axs.legend()\n",
    "    axs.grid(visible = True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Gradient norm (notebook implementation): {np.linalg.norm(grad_local - grad_sp_vals)}\")\n",
    "print(f\"Gradient norm (package implementation): {np.linalg.norm(grad_code - grad_sp_vals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c6bfb-e369-4412-906d-c803c584d717",
   "metadata": {},
   "source": [
    "## Check the Hessian matrices for the three implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2cc72b-a3a8-4cb5-bcee-4a6330e42b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hess_local = h_hess(D_matrix, w_i, w_j)\n",
    "hess_code = spectral_reg.spectral_hessian(w_i, w_j)\n",
    "hess_sp_vals = hess_sp.subs(vals)\n",
    "hess_sp_vals = np.array(hess_sp_vals).astype(np.float64)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize = (14, 8))\n",
    "fig.suptitle(\"Hessian matrices\")\n",
    "ax[0].set_title(\"Notebook implementation\")\n",
    "ax[1].set_title(\"Package implementation\")\n",
    "ax[2].set_title(\"SymPy implementation\")\n",
    "\n",
    "m1 = ax[0].imshow(hess_local)\n",
    "m2 = ax[1].imshow(hess_code)\n",
    "m3 = ax[2].imshow(np.array(hess_sp_vals).astype(np.float64))\n",
    "\n",
    "for axs, ms in zip(ax, [m1, m2, m3]):\n",
    "    plt.colorbar(ms, ax = axs, fraction=0.047)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14, 8))\n",
    "fig.suptitle(\"Hessian Per-index errors\")\n",
    "ax[0].set_title(\"Notebook implementation\")\n",
    "ax[1].set_title(\"Package implementation\")\n",
    "\n",
    "m1 = ax[0].imshow(hess_local - hess_sp_vals)\n",
    "m2 = ax[1].imshow(hess_code - hess_sp_vals)\n",
    "\n",
    "for axs, ms in zip(ax, [m1, m2]):\n",
    "    plt.colorbar(ms, ax = axs, fraction=0.047)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Hessian matrix norm (notebook implementation): {np.linalg.norm(hess_local - hess_sp_vals)}\")\n",
    "print(f\"Hessian matrix norm (package implementation): {np.linalg.norm(hess_code - hess_sp_vals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9e039-f2f5-4af1-8479-b3e705404e55",
   "metadata": {},
   "source": [
    "# Final example: Newton's method using the notebook implementation\n",
    "\n",
    "In this example, we will look at what happens to the $\\mathbf{w}_i$ vector if it is enforced to be orthogonal to the $\\mathbf{w}_j$ vector. As the $\\mathbf{w}_j$ vector is active everywhere, it is expected that performing Newton iteration will reduce the vector to zero vector. \n",
    "\n",
    "Note that as the Hessian matrix is no longer a function of our parameter vector $\\mathbf{w}_i$, it is expected that we get to the solution in 1 iteration.\n",
    "\n",
    "This is an extreme example, as it is not expected that the $\\mathbf{w}_j$ excite all frequencies equally, but it ensures that some logic of the problem is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d1ba5-73cf-4219-9c40-e351d676d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_i_newton = w_i.copy()\n",
    "n_iters = 10\n",
    "update_norm = np.zeros(n_iters)\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    grad = h_grad(D_matrix, w_i_newton, w_j)\n",
    "    hess = h_hess(D_matrix, w_i_newton, w_j)\n",
    "\n",
    "    # Compute update\n",
    "    delta = np.linalg.solve(hess, grad)\n",
    "    \n",
    "    # Adjust w_i_newton\n",
    "    w_i_newton -= delta\n",
    "\n",
    "    # Store norm of update\n",
    "    update_norm[i] = np.linalg.norm(delta)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(update_norm, color = \"b\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Update norm\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 6))\n",
    "fig.suptitle(r\"Visualise the vectors $\\mathbf{w}_i$ and $\\mathbf{w}_j$.\")\n",
    "ax[0].plot(w_i_newton[:, 0], color = \"b\", label = r\"$\\mathbf{w}_i$\")\n",
    "ax[1].plot(w_j[:, 0], color = \"r\", label = r\"$\\mathbf{w}_j$\")\n",
    "\n",
    "for axs in ax:\n",
    "    axs.grid()\n",
    "    axs.set_xlabel(\"Vector indices\")\n",
    "    axs.grid(visible = True)\n",
    "    axs.legend()\n",
    "    axs.set_ylim(-2, 2)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 6))\n",
    "fig.suptitle(r\"Visualise the spectral information in the vectors $\\mathbf{w}_i$ and $\\mathbf{w}_j$.\")\n",
    "ax[0].plot(*fft_vis(w_i_newton[:, 0], Fs), color = \"b\", label = r\"$\\mathbf{w}_i$\")\n",
    "ax[1].plot(*fft_vis(w_j[:, 0], Fs), color = \"r\", label = r\"$\\mathbf{w}_j$\")\n",
    "\n",
    "for axs in ax:\n",
    "    axs.grid()\n",
    "    axs.set_xlabel(\"Frequency\")\n",
    "    axs.set_ylabel(\"Magnitude\")\n",
    "    axs.grid(visible = True)\n",
    "    axs.legend()\n",
    "    axs.set_ylim(-0.5, 2)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253dac87-1e93-4c7b-86e8-ad7f7854d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the values of the w_i vector (expected to be numerically zero):\n",
    "w_i_newton.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e75f2-dbc9-40a8-b4ed-86f412f07292",
   "metadata": {},
   "source": [
    "Hence, we can see that the updated $\\mathbf{w}_i$ vector is numerically zero, which is expected given that $\\mathbf{w}_j$ is a broadband filter that excites all frequencies equally. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
